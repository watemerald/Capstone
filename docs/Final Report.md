# CSC-690 Graduate Capstone in Computer Science

## Improving Video Search with Automated Tagging

Final Report

By: Andrii Sherman

Under the guidance of: Dr. Saleh Aliyari

College of Art and Sciences
Adelphi University, Garden City, New York

## Abstract

Search has been an important part of accessing information on the web since the internet has existed. Starting from simple catalogues and relevancy term searches to the ranking methods which display the most relevant information to the search user. The problem for many of those situations has been that it was only suitable for searching and indexing of text data. As more information is getting shared in video form, searching for that becomes harder. The main source of that information has been text information: video title, description, tags. With the rise of computer vision it is now possible to extract information from images, specifically video frames, and use that information for search instead.
In this project, I make use of YouTube-8M, a video classification benchmark, to get appropriate video labels, and use them to construct a video search engine. I will compare how well the search performs using only the user-entered tags, the autogenerated labels, and both together, and propose whether automated tagging system could be used to improve video search in general.

## Background

Automated tagging has rapidly improved image search, with massive collections such as ImageNet being attributed to the increase in quality of automated tagging. These sorts of systems have improved the ability for people to search for images, as automated tagging greatly improves the effectiveness, because without them the only information that could actually be searched is the metadata associated with the image. Automated image tagging therefore increases the amount of available metadata by describing what is in the image.

While such improvements have been happening for images for quite some time, it has been a longer time for such collections to come for videos. Like for images, automated tagging of videos only started to take off after large-scale datasets were made available. The dataset used in this paper, YouTube-8M contains ~8 million videos (500k hours of video) that is annotated with 4800 visual entities. This dataset has been the target of 3 challenges, hosted by Google, that has provided increased understanding of how to effectively work automated video tagging.

I've done previous work on this exact topic back in 2016, before YouTube-8M was released. The biggest problems with my work from that time had been with a lack of any publicly available tagged video dataset to be used for evaluating my model, and as such most of the time back then had to be devoted to gathering and tagging video files and keyframes. Because YouTube-8M now exists there is now no need to go through the same process, and my work can continue from evaluating whether the use of automated tagging.

## Project Description

The goal of this project is evaluating whether the use of the use of automated video tagging can be used to improve video search. This can be separated into 2 distinct parts:

1. Efficient tagging of videos based on frame-level information

   YouTube-8M provides 2 different levels of models - a frame-level one, and a video-level one. The frame-level model is created based on feature extraction from images, in this case frames of a video. By using this frame information efficient labels can be created for the whole video. This can then be evaluated against the labels provided by YouTube-8M.

2. Determine if search is improved by the automatic tagging

   After generating video-level tags I determine if their use can improve video search.

## Project Implementation

Overall the project was a success. It proceeded quite smoothly, with a successfully built model that managed to outperform the user-submitted tags in both precision and recall on the final search evaluation.

The main challenges faced were dealing with huge amounts of data, as just reading the whole training dataset took more than 9 minutes. These problems had me rethink how the data should be loaded, as even the smaller video-level feature dataset was too big to store in system memory, resulting in having to load it from disk each time it had to be accessed. In order to continue working on this problem this part of the code needs to be rewritten to minimize the effects of disk read speeds. In particular, parallel reads to pre-fetch the data into RAM could save over 2 hours on training a full model.

Other challenges faced included working on loading the model into VRAM, as even small batches required more memory than the video card I trained it on had.

### Sprint 1

The first sprint was focused on getting to know the dataset and the papers on video classification.

The main goals were:

- To be able to process the stored dataset, as well as write the data loader to be able to do batch training and predictions further on.
- To explore the state of the art in video classification, and settle on the models to implement in further sprints. In going over the papers submitted to the YouTube-8m Video Understanding challenges (2017,2018,2019) I settled on 2 models to implement - a simple baseline model, and a Gated NetVLAD model - the best performing single (non-ensemble) model from the 2017 challenge.

### Sprint 2

The second sprint had to do with writing the base code needed to train models and make predictions on them, afterwards testing them by implementing the simple baseline model.

### Sprint 3

The third sprint tackled the last programming challenges left in implementing the models. During it I:

- Created a CLI interface to more easily launch and manage training and prediction runs.
- Implemented the Gated NetVLAD model.
- Refactored the shared code to avoid duplication between the 2 models.
- Created a pipeline using the Mediapipe YouTube-8M feature extractor to make predictions on arbitrary videos.
- Created a simple GUI to show how the program works.

### Sprint 4

For the final sprint the goal was to evaluate how well the program performed.

- Fully trained the model.
- Created a sample of evaluation videos.
- Worked with the YouTube API in order to get the user-submitted tags for them.
- Generated the tags using the trained model.
- Made use of Elasticsearch to evaluate how well the 3 sets of tags considered performed against the dataset-provided tags.

## Results and Future Plans

The autogenerated tags performed remarkably well. They managed a precision of 46.7% and recall of 50.0% against the user-provided YouTube tags, which got a precision of 28.3% and recall of 31.1%. Combining them together lead to a worse score than just the predicted tags, with recall of 37.8% and precision 50.0%.

These numbers are promising for the future use of autogenerated tags in video search, but more research is required into understanding just how useful it would be.
While they outperformed randomly sampled videos, it might be that a different subset selected with more intent (for instance, videos uploaded after 2015, or videos with at least 10000 views) might still outperform the autogenerated tags. Although doing such will require access to the YouTube API without its imposed limits, in order to avoid having to gather information for almost a year to stay within the daily limits.

A further improvement on these results might come as a result of implementing a state-of-the art video classification model (ensemble of models) and using that to make more accurate predictions.

## Sources

## Dataset

- [YouTube-8M: A large and Diverse Labeled Video Dataset for Video Understanding Research](https://research.google.com/youtube8m/download.html)
- Kaggle YouTube-8M Video Understanding Challenge: [1](https://www.kaggle.com/c/youtube8m), [2](https://www.kaggle.com/c/youtube8m-2018), [3](https://www.kaggle.com/c/youtube8m-2019)
- [YouTube-8M 2019 Starter Kernel](https://www.kaggle.com/inversion/starter-kernel-yt8m-2019-sample-data)
- [YouTube-8M Starter code](https://github.com/google/youtube-8m/)

## Tensorflow

- [Official TensorFlow Tutorials](https://www.tensorflow.org/tutorials)
- [Introduction to TensorFlow for Artificial Intelligence, Machine Learning, and Deep Learning by deeplearning.ai](https://www.coursera.org/learn/introduction-tensorflow/home/welcome)
- [Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition](https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)
- [Deep Learning with TensorFlow 2 and Keras - Second Edition](https://learning.oreilly.com/library/view/deep-learning-with/9781838823412/)

## Video Classification models

- Miech, Antoine, Ivan Laptev, and Josef Sivic. "Learnable pooling with context gating for video classification." arXiv preprint arXiv:1706.06905 (2017).
- Arandjelovic, Relja, et al. "NetVLAD: CNN architecture for weakly supervised place recognition." Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.
- Liu, Tianqi, and Bo Liu. "Constrained-size tensorflow models for youtube-8m video understanding challenge." Proceedings of the European Conference on Computer Vision (ECCV). 2018.
- [Keras baseline on video features](https://www.kaggle.com/drn01z3/keras-baseline-on-video-features-0-7941-lb/code)
- [Learnable mOdUle for Pooling fEatures (LOUPE) Tensorflow Toolbox](https://github.com/antoine77340/LOUPE)
- [Mixture of experts layers for Keras](https://github.com/eminorhan/mixture-of-experts)

## Utilities

- [YouTube-8M Feature Extraction and Model Inference MediaPipe](https://google.github.io/mediapipe/solutions/youtube_8m.html)
- [youtube-dl](https://ytdl-org.github.io/youtube-dl/index.html)
- [Elasticsearch](https://www.elastic.co/)
- [NumPy](https://numpy.org/)
- [pandas](https://pandas.pydata.org/)
- [streamlit](https://www.streamlit.io/)
